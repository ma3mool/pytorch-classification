{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training script for CIFAR-10/100\n",
    "Copyright (c) Wei YANG, 2017\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models.cifar as models\n",
    "\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "class Args:\n",
    "    dataset = 'cifar100'\n",
    "    workers = 12\n",
    "    epochs = 164\n",
    "    start_epoch = 0\n",
    "    train_batch = 128\n",
    "    test_batch = 100\n",
    "    lr = 0.1\n",
    "    drop = 0\n",
    "    schedule = [81, 122]\n",
    "    gamma = 0.1\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    checkpoint = 'checkpoint'\n",
    "    resume = ''\n",
    "    arch = 'alexnet'\n",
    "    depth = 29\n",
    "    block_name = 'BasicBlock'\n",
    "    cardinality = 8\n",
    "    widen_factor = 4\n",
    "    growthRate = 12\n",
    "    compressionRate = 2\n",
    "    manualSeed =  None\n",
    "    evaluate =  False #** action 'store_true'; help='evaluate model on validation set'\n",
    "    gpu_id = '0'\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "args=Args()\n",
    "state = {\n",
    "    \"dataset\"         : args.dataset,\n",
    "    \"workers\"         : args.workers,\n",
    "    \"epochs\"          : args.epochs,\n",
    "    \"start_epoch\"     : args.start_epoch,\n",
    "    \"train_batch\"     : args.train_batch,\n",
    "    \"test_batch\"      : args.test_batch,\n",
    "    \"lr\"              : args.lr,\n",
    "    \"drop\"            : args.drop,\n",
    "    \"schedule\"        : args.schedule,\n",
    "    \"gamma\"           : args.gamma,\n",
    "    \"momentum\"        : args.momentum,\n",
    "    \"weight_decay\"    : args.weight_decay,\n",
    "    \"checkpoint\"      : args.checkpoint,\n",
    "    \"resume\"          : args.resume,\n",
    "    \"arch\"            : args.arch,\n",
    "    \"depth\"           : args.depth,\n",
    "    \"block_name\"      : args.block_name,\n",
    "    \"cardinality\"     : args.cardinality,\n",
    "    \"widen_factor\"    : args.widen_factor,\n",
    "    \"growthRate\"      : args.growthRate,\n",
    "    \"compressionRate\" : args.compressionRate,\n",
    "    \"anualSeed\"       : args.manualSeed,\n",
    "    \"evaluate\"        : args.evaluate,\n",
    "    \"gpu_id\"          : args.gpu_id,\n",
    "}\n",
    "\n",
    "# Validate dataset\n",
    "assert args.dataset == 'cifar10' or args.dataset == 'cifar100', 'Dataset can only be cifar10 or cifar100.'\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "\n",
    "# Random seed\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(testloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(testloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in args.schedule:\n",
    "        state['lr'] *= args.gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_acc\n",
    "    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "    if not os.path.isdir(args.checkpoint):\n",
    "        mkdir_p(args.checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "    # Data\n",
    "    print('==> Preparing dataset %s' % args.dataset)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    if args.dataset == 'cifar10':\n",
    "        dataloader = datasets.CIFAR10\n",
    "        num_classes = 10\n",
    "    else:\n",
    "        dataloader = datasets.CIFAR100\n",
    "        num_classes = 100\n",
    "\n",
    "\n",
    "    trainset = dataloader(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)\n",
    "\n",
    "    testset = dataloader(root='./data', train=False, download=False, transform=transform_test)\n",
    "    testloader = data.DataLoader(testset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "    # Model\n",
    "    print(\"==> creating model '{}'\".format(args.arch))\n",
    "    if args.arch.startswith('resnext'):\n",
    "        model = models.__dict__[args.arch](\n",
    "                    cardinality=args.cardinality,\n",
    "                    num_classes=num_classes,\n",
    "                    depth=args.depth,\n",
    "                    widen_factor=args.widen_factor,\n",
    "                    dropRate=args.drop,\n",
    "                )\n",
    "    elif args.arch.startswith('densenet'):\n",
    "        model = models.__dict__[args.arch](\n",
    "                    num_classes=num_classes,\n",
    "                    depth=args.depth,\n",
    "                    growthRate=args.growthRate,\n",
    "                    compressionRate=args.compressionRate,\n",
    "                    dropRate=args.drop,\n",
    "                )\n",
    "    elif args.arch.startswith('wrn'):\n",
    "        model = models.__dict__[args.arch](\n",
    "                    num_classes=num_classes,\n",
    "                    depth=args.depth,\n",
    "                    widen_factor=args.widen_factor,\n",
    "                    dropRate=args.drop,\n",
    "                )\n",
    "    elif args.arch.endswith('resnet'):\n",
    "        model = models.__dict__[args.arch](\n",
    "                    num_classes=num_classes,\n",
    "                    depth=args.depth,\n",
    "                    block_name=args.block_name,\n",
    "                )\n",
    "    else:\n",
    "        model = models.__dict__[args.arch](num_classes=num_classes)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        \n",
    "    cudnn.benchmark = True\n",
    "    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Resume\n",
    "    title = 'cifar-10-' + args.arch\n",
    "    if args.resume:\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isfile(args.resume), 'Error: no checkpoint directory found!'\n",
    "        args.checkpoint = os.path.dirname(args.resume)\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title, resume=True)\n",
    "    else:\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title)\n",
    "        logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])\n",
    "\n",
    "\n",
    "    if args.evaluate:\n",
    "        print('\\nEvaluation only')\n",
    "        test_loss, test_acc = test(testloader, model, criterion, start_epoch, use_cuda)\n",
    "        print(' Test Loss:  %.8f, Test Acc:  %.2f' % (test_loss, test_acc))\n",
    "        return\n",
    "\n",
    "    # Train and val\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, state['lr']))\n",
    "\n",
    "        train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, use_cuda)\n",
    "        test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "\n",
    "        # append logger file\n",
    "        logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "\n",
    "        # save model\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best, checkpoint=args.checkpoint)\n",
    "\n",
    "    logger.close()\n",
    "    logger.plot()\n",
    "    savefig(os.path.join(args.checkpoint, 'log.eps'))\n",
    "\n",
    "    print('Best acc:')\n",
    "    print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
